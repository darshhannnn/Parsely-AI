# Production Docker Compose for Parsely AI
version: '3.8'

services:
  # Main API Service
  parsely-api:
    build: .
    container_name: parsely-ai
    ports:
      - "8000:8000"
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - LLM_MODEL=gemini-1.5-flash
      - LLM_PROVIDER=google
      - LLM_TEMPERATURE=0.1
      - POLICY_DOCUMENTS_PATH=/app/data/policies
      - EMBEDDINGS_PATH=/app/data/embeddings
      - MAX_CONCURRENT_REQUESTS=20
      - CACHE_TTL_SECONDS=3600
      - ENABLE_QUERY_CACHING=true
    volumes:
      - ./data/policies:/app/data/policies:ro
      - ./data/embeddings:/app/data/embeddings
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: parsely-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - parsely-api
    restart: unless-stopped

  # Redis for Caching (Optional)
  redis:
    image: redis:7-alpine
    container_name: llm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

  # Monitoring with Prometheus (Optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: llm-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    restart: unless-stopped

volumes:
  redis_data:
  prometheus_data: